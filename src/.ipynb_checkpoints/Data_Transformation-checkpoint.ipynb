{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+------+---------+---------+-----------+-----------+--------+----------+-----+--------+---+--------+---------+----------+------+---------+\n",
      "|Private|Apps|Accept|Enroll|Top10perc|Top25perc|F_Undergrad|P_Undergrad|Outstate|Room_Board|Books|Personal|PhD|Terminal|S_F_Ratio|perc_alumi|Expend|Grad_Rate|\n",
      "+-------+----+------+------+---------+---------+-----------+-----------+--------+----------+-----+--------+---+--------+---------+----------+------+---------+\n",
      "|    Yes|1660|  1232|   721|       23|       52|       2885|        537|    7440|      3300|  450|    2200| 70|      78|     18.1|        12|  7041|       60|\n",
      "|    Yes|2186|  1924|   512|       16|       29|       2683|       1227|   12280|      6450|  750|    1500| 29|      30|     12.2|        16| 10527|       56|\n",
      "|    Yes|1428|  1097|   336|       22|       50|       1036|         99|   11250|      3750|  400|    1165| 53|      66|     12.9|        30|  8735|       54|\n",
      "|    Yes| 417|   349|   137|       60|       89|        510|         63|   12960|      5450|  450|     875| 92|      97|      7.7|        37| 19016|       59|\n",
      "|    Yes| 193|   146|    55|       16|       44|        249|        869|    7560|      4120|  800|    1500| 76|      72|     11.9|         2| 10922|       15|\n",
      "|    Yes| 587|   479|   158|       38|       62|        678|         41|   13500|      3335|  500|     675| 67|      73|      9.4|        11|  9727|       55|\n",
      "|    Yes| 353|   340|   103|       17|       45|        416|        230|   13290|      5720|  500|    1500| 90|      93|     11.5|        26|  8861|       63|\n",
      "|    Yes|1899|  1720|   489|       37|       68|       1594|         32|   13868|      4826|  450|     850| 89|     100|     13.7|        37| 11487|       73|\n",
      "|    Yes|1038|   839|   227|       30|       63|        973|        306|   15595|      4400|  300|     500| 79|      84|     11.3|        23| 11644|       80|\n",
      "|    Yes| 582|   498|   172|       21|       44|        799|         78|   10468|      3380|  660|    1800| 40|      41|     11.5|        15|  8991|       52|\n",
      "|    Yes|1732|  1425|   472|       37|       75|       1830|        110|   16548|      5406|  500|     600| 82|      88|     11.3|        31| 10932|       73|\n",
      "|    Yes|2652|  1900|   484|       44|       77|       1707|         44|   17080|      4440|  400|     600| 73|      91|      9.9|        41| 11711|       76|\n",
      "|    Yes|1179|   780|   290|       38|       64|       1130|        638|    9690|      4785|  600|    1000| 60|      84|     13.3|        21|  7940|       74|\n",
      "|    Yes|1267|  1080|   385|       44|       73|       1306|         28|   12572|      4552|  400|     400| 79|      87|     15.3|        32|  9305|       68|\n",
      "|    Yes| 494|   313|   157|       23|       46|       1317|       1235|    8352|      3640|  650|    2449| 36|      69|     11.1|        26|  8127|       55|\n",
      "|    Yes|1420|  1093|   220|        9|       22|       1018|        287|    8700|      4780|  450|    1400| 78|      84|     14.7|        19|  7355|       69|\n",
      "|    Yes|4302|   992|   418|       83|       96|       1593|          5|   19760|      5300|  660|    1598| 93|      98|      8.4|        63| 21424|      100|\n",
      "|    Yes|1216|   908|   423|       19|       40|       1819|        281|   10100|      3520|  550|    1100| 48|      61|     12.1|        14|  7994|       59|\n",
      "|    Yes|1130|   704|   322|       14|       23|       1586|        326|    9996|      3090|  900|    1320| 62|      66|     11.5|        18| 10908|       46|\n",
      "|     No|3540|  2001|  1016|       24|       54|       4190|       1512|    5130|      3592|  500|    2000| 60|      62|     23.1|         5|  4010|       34|\n",
      "+-------+----+------+------+---------+---------+-----------+-----------+--------+----------+-----+--------+---+--------+---------+----------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.appName('Data_Understanding').getOrCreate()\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pandasDF = pd.read_csv('/home/ubuntu/BDAS_yang124/Datasets/College.csv')\n",
    "sparkDF = spark.createDataFrame(pandasDF)\n",
    "df = sparkDF.selectExpr(\n",
    " 'Private',\n",
    " 'Apps',\n",
    " 'Accept',\n",
    " 'Enroll',\n",
    " 'Top10perc',\n",
    " 'Top25perc',\n",
    " \"`F.Undergrad` as F_Undergrad\",\n",
    " \"`P.Undergrad` as P_Undergrad\",\n",
    " 'Outstate',\n",
    " \"`Room.Board` as Room_Board\",\n",
    " 'Books',\n",
    " 'Personal',\n",
    " 'PhD',\n",
    " 'Terminal',\n",
    " \"`S.F.Ratio` as S_F_Ratio\",\n",
    " \"`perc.alumni` as perc_alumi\",\n",
    " 'Expend',\n",
    "\"`Grad.Rate` as Grad_Rate\" )\n",
    "df.show()\n",
    "# df.withColumn('Id_New',when(df.Rank <= 5,df.Id).otherwise('other')).drop(df.Id).select(col('Id_New').alias('Id'),col('Rank')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apps done\n",
      "Accept done\n",
      "Enroll done\n",
      "Top10perc done\n",
      "Top25perc done\n",
      "F_Undergrad done\n",
      "P_Undergrad done\n",
      "Outstate done\n",
      "Room_Board done\n",
      "Books done\n",
      "Personal done\n",
      "PhD done\n",
      "Terminal done\n",
      "S_F_Ratio done\n",
      "perc_alumi done\n",
      "Expend done\n",
      "Grad_Rate done\n",
      "{'Enroll': [35.0, 6392.0], 'Grad_Rate': [10.0, 118.0], 'perc_alumi': [0.0, 64.0], 'Room_Board': [1780.0, 8124.0], 'F_Undergrad': [139.0, 31643.0], 'Expend': [3186.0, 56233.0], 'Top10perc': [1.0, 96.0], 'Accept': [72.0, 26330.0], 'P_Undergrad': [1.0, 21836.0], 'Personal': [250.0, 6800.0], 'Books': [96.0, 2340.0], 'S_F_Ratio': [2.5, 39.8], 'Terminal': [24.0, 100.0], 'Top25perc': [9.0, 100.0], 'Apps': [81.0, 48094.0], 'PhD': [8.0, 103.0], 'Outstate': [2340.0, 21700.0]}\n",
      "Apps done\n",
      "Accept done\n",
      "Enroll done\n",
      "Top10perc done\n",
      "Top25perc done\n",
      "F_Undergrad done\n",
      "P_Undergrad done\n",
      "Outstate done\n",
      "Room_Board done\n",
      "Books done\n",
      "Personal done\n",
      "PhD done\n",
      "Terminal done\n",
      "S_F_Ratio done\n",
      "perc_alumi done\n",
      "Expend done\n",
      "Grad_Rate done\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`pmid`' given input columns: [Terminal, Enroll, Apps, PhD, scaledFeatures, S_F_Ratio, Room_Board, F_Undergrad, P_Undergrad, features, Expend, Outstate, Top25perc, Top10perc, Personal, Grad_Rate, Books, perc_alumi, Accept];;\\n'Project ['pmid, scaledFeatures#6554]\\n+- Project [Apps#41L, Accept#42L, Enroll#43L, Top10perc#44L, Top25perc#45L, F_Undergrad#78L, P_Undergrad#79L, Outstate#48L, Room_Board#80L, Books#50L, Personal#51L, PhD#52L, Terminal#53L, S_F_Ratio#81, perc_alumi#82L, Expend#56L, Grad_Rate#6495, features#6529, UDF(features#6529) AS scaledFeatures#6554]\\n   +- Project [Apps#41L, Accept#42L, Enroll#43L, Top10perc#44L, Top25perc#45L, F_Undergrad#78L, P_Undergrad#79L, Outstate#48L, Room_Board#80L, Books#50L, Personal#51L, PhD#52L, Terminal#53L, S_F_Ratio#81, perc_alumi#82L, Expend#56L, Grad_Rate#6495, UDF(named_struct(Apps_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(Apps#41L as double), Accept_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(Accept#42L as double), Enroll_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(Enroll#43L as double), Top10perc_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(Top10perc#44L as double), Top25perc_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(Top25perc#45L as double), F_Undergrad_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(F_Undergrad#78L as double), P_Undergrad_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(P_Undergrad#79L as double), Outstate_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(Outstate#48L as double), Room_Board_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(Room_Board#80L as double), Books_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(Books#50L as double), Personal_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(Personal#51L as double), PhD_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(PhD#52L as double), ... 10 more fields)) AS features#6529]\\n      +- Project [Apps#41L, Accept#42L, Enroll#43L, Top10perc#44L, Top25perc#45L, F_Undergrad#78L, P_Undergrad#79L, Outstate#48L, Room_Board#80L, Books#50L, Personal#51L, PhD#52L, Terminal#53L, S_F_Ratio#81, perc_alumi#82L, Expend#56L, LOG((CASE WHEN (cast(Grad_Rate#83L as double) < 10.0) THEN 10.0 WHEN (cast(Grad_Rate#83L as double) > 118.0) THEN 118.0 ELSE cast(Grad_Rate#83L as double) END + cast(1 as double))) AS Grad_Rate#6495]\\n         +- Project [Apps#41L, Accept#42L, Enroll#43L, Top10perc#44L, Top25perc#45L, F_Undergrad#78L, P_Undergrad#79L, Outstate#48L, Room_Board#80L, Books#50L, Personal#51L, PhD#52L, Terminal#53L, S_F_Ratio#81, perc_alumi#82L, Expend#56L, Grad_Rate#83L]\\n            +- Project [Private#40, Apps#41L, Accept#42L, Enroll#43L, Top10perc#44L, Top25perc#45L, F.Undergrad#46L AS F_Undergrad#78L, P.Undergrad#47L AS P_Undergrad#79L, Outstate#48L, Room.Board#49L AS Room_Board#80L, Books#50L, Personal#51L, PhD#52L, Terminal#53L, S.F.Ratio#54 AS S_F_Ratio#81, perc.alumni#55L AS perc_alumi#82L, Expend#56L, Grad.Rate#57L AS Grad_Rate#83L]\\n               +- LogicalRDD [CN#39, Private#40, Apps#41L, Accept#42L, Enroll#43L, Top10perc#44L, Top25perc#45L, F.Undergrad#46L, P.Undergrad#47L, Outstate#48L, Room.Board#49L, Books#50L, Personal#51L, PhD#52L, Terminal#53L, S.F.Ratio#54, perc.alumni#55L, Expend#56L, Grad.Rate#57L]\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4631.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`pmid`' given input columns: [Terminal, Enroll, Apps, PhD, scaledFeatures, S_F_Ratio, Room_Board, F_Undergrad, P_Undergrad, features, Expend, Outstate, Top25perc, Top10perc, Personal, Grad_Rate, Books, perc_alumi, Accept];;\n'Project ['pmid, scaledFeatures#6554]\n+- Project [Apps#41L, Accept#42L, Enroll#43L, Top10perc#44L, Top25perc#45L, F_Undergrad#78L, P_Undergrad#79L, Outstate#48L, Room_Board#80L, Books#50L, Personal#51L, PhD#52L, Terminal#53L, S_F_Ratio#81, perc_alumi#82L, Expend#56L, Grad_Rate#6495, features#6529, UDF(features#6529) AS scaledFeatures#6554]\n   +- Project [Apps#41L, Accept#42L, Enroll#43L, Top10perc#44L, Top25perc#45L, F_Undergrad#78L, P_Undergrad#79L, Outstate#48L, Room_Board#80L, Books#50L, Personal#51L, PhD#52L, Terminal#53L, S_F_Ratio#81, perc_alumi#82L, Expend#56L, Grad_Rate#6495, UDF(named_struct(Apps_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(Apps#41L as double), Accept_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(Accept#42L as double), Enroll_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(Enroll#43L as double), Top10perc_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(Top10perc#44L as double), Top25perc_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(Top25perc#45L as double), F_Undergrad_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(F_Undergrad#78L as double), P_Undergrad_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(P_Undergrad#79L as double), Outstate_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(Outstate#48L as double), Room_Board_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(Room_Board#80L as double), Books_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(Books#50L as double), Personal_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(Personal#51L as double), PhD_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(PhD#52L as double), ... 10 more fields)) AS features#6529]\n      +- Project [Apps#41L, Accept#42L, Enroll#43L, Top10perc#44L, Top25perc#45L, F_Undergrad#78L, P_Undergrad#79L, Outstate#48L, Room_Board#80L, Books#50L, Personal#51L, PhD#52L, Terminal#53L, S_F_Ratio#81, perc_alumi#82L, Expend#56L, LOG((CASE WHEN (cast(Grad_Rate#83L as double) < 10.0) THEN 10.0 WHEN (cast(Grad_Rate#83L as double) > 118.0) THEN 118.0 ELSE cast(Grad_Rate#83L as double) END + cast(1 as double))) AS Grad_Rate#6495]\n         +- Project [Apps#41L, Accept#42L, Enroll#43L, Top10perc#44L, Top25perc#45L, F_Undergrad#78L, P_Undergrad#79L, Outstate#48L, Room_Board#80L, Books#50L, Personal#51L, PhD#52L, Terminal#53L, S_F_Ratio#81, perc_alumi#82L, Expend#56L, Grad_Rate#83L]\n            +- Project [Private#40, Apps#41L, Accept#42L, Enroll#43L, Top10perc#44L, Top25perc#45L, F.Undergrad#46L AS F_Undergrad#78L, P.Undergrad#47L AS P_Undergrad#79L, Outstate#48L, Room.Board#49L AS Room_Board#80L, Books#50L, Personal#51L, PhD#52L, Terminal#53L, S.F.Ratio#54 AS S_F_Ratio#81, perc.alumni#55L AS perc_alumi#82L, Expend#56L, Grad.Rate#57L AS Grad_Rate#83L]\n               +- LogicalRDD [CN#39, Private#40, Apps#41L, Accept#42L, Enroll#43L, Top10perc#44L, Top25perc#45L, F.Undergrad#46L, P.Undergrad#47L, Outstate#48L, Room.Board#49L, Books#50L, Personal#51L, PhD#52L, Terminal#53L, S.F.Ratio#54, perc.alumni#55L, Expend#56L, Grad.Rate#57L]\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:83)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:290)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:290)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:266)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:276)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:280)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:280)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:285)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:285)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:83)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:128)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:63)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2845)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1131)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8d02b9adc4a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpmid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaledFeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mfinal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaledData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pmid\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"scaledFeatures\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mfinal_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \"\"\"\n\u001b[0;32m--> 993\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`pmid`' given input columns: [Terminal, Enroll, Apps, PhD, scaledFeatures, S_F_Ratio, Room_Board, F_Undergrad, P_Undergrad, features, Expend, Outstate, Top25perc, Top10perc, Personal, Grad_Rate, Books, perc_alumi, Accept];;\\n'Project ['pmid, scaledFeatures#6554]\\n+- Project [Apps#41L, Accept#42L, Enroll#43L, Top10perc#44L, Top25perc#45L, F_Undergrad#78L, P_Undergrad#79L, Outstate#48L, Room_Board#80L, Books#50L, Personal#51L, PhD#52L, Terminal#53L, S_F_Ratio#81, perc_alumi#82L, Expend#56L, Grad_Rate#6495, features#6529, UDF(features#6529) AS scaledFeatures#6554]\\n   +- Project [Apps#41L, Accept#42L, Enroll#43L, Top10perc#44L, Top25perc#45L, F_Undergrad#78L, P_Undergrad#79L, Outstate#48L, Room_Board#80L, Books#50L, Personal#51L, PhD#52L, Terminal#53L, S_F_Ratio#81, perc_alumi#82L, Expend#56L, Grad_Rate#6495, UDF(named_struct(Apps_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(Apps#41L as double), Accept_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(Accept#42L as double), Enroll_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(Enroll#43L as double), Top10perc_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(Top10perc#44L as double), Top25perc_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(Top25perc#45L as double), F_Undergrad_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(F_Undergrad#78L as double), P_Undergrad_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(P_Undergrad#79L as double), Outstate_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(Outstate#48L as double), Room_Board_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(Room_Board#80L as double), Books_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(Books#50L as double), Personal_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(Personal#51L as double), PhD_double_VectorAssembler_460e90a0685f6d8a1cf2, cast(PhD#52L as double), ... 10 more fields)) AS features#6529]\\n      +- Project [Apps#41L, Accept#42L, Enroll#43L, Top10perc#44L, Top25perc#45L, F_Undergrad#78L, P_Undergrad#79L, Outstate#48L, Room_Board#80L, Books#50L, Personal#51L, PhD#52L, Terminal#53L, S_F_Ratio#81, perc_alumi#82L, Expend#56L, LOG((CASE WHEN (cast(Grad_Rate#83L as double) < 10.0) THEN 10.0 WHEN (cast(Grad_Rate#83L as double) > 118.0) THEN 118.0 ELSE cast(Grad_Rate#83L as double) END + cast(1 as double))) AS Grad_Rate#6495]\\n         +- Project [Apps#41L, Accept#42L, Enroll#43L, Top10perc#44L, Top25perc#45L, F_Undergrad#78L, P_Undergrad#79L, Outstate#48L, Room_Board#80L, Books#50L, Personal#51L, PhD#52L, Terminal#53L, S_F_Ratio#81, perc_alumi#82L, Expend#56L, Grad_Rate#83L]\\n            +- Project [Private#40, Apps#41L, Accept#42L, Enroll#43L, Top10perc#44L, Top25perc#45L, F.Undergrad#46L AS F_Undergrad#78L, P.Undergrad#47L AS P_Undergrad#79L, Outstate#48L, Room.Board#49L AS Room_Board#80L, Books#50L, Personal#51L, PhD#52L, Terminal#53L, S.F.Ratio#54 AS S_F_Ratio#81, perc.alumni#55L AS perc_alumi#82L, Expend#56L, Grad.Rate#57L AS Grad_Rate#83L]\\n               +- LogicalRDD [CN#39, Private#40, Apps#41L, Accept#42L, Enroll#43L, Top10perc#44L, Top25perc#45L, F.Undergrad#46L, P.Undergrad#47L, Outstate#48L, Room.Board#49L, Books#50L, Personal#51L, PhD#52L, Terminal#53L, S.F.Ratio#54, perc.alumni#55L, Expend#56L, Grad.Rate#57L]\\n\""
     ]
    }
   ],
   "source": [
    "d = {}\n",
    "# Fill in the entries one by one\n",
    "df = df.drop('CN','Private')\n",
    "for col in df.columns:\n",
    "      d[col] = df.approxQuantile(col,[0.25,0.75],0.25)\n",
    "      print(col+\" done\")\n",
    "    \n",
    "print(d)\n",
    "for col in df.columns:\n",
    "    df_new = df.withColumn(col, \\\n",
    "    F.log(F.when(df[col] < d[col][0],d[col][0])\\\n",
    "    .when(df[col] > d[col][1], d[col][1])\\\n",
    "    .otherwise(df[col] ) +1).alias(col))\n",
    "    print(col+\" done\")\n",
    "\n",
    "df_new.show()\n",
    "# assembler = VectorAssembler().setInputCols(df_new.columns).setOutputCol(\"features\")\n",
    "# transformed = assembler.transform(df_new)\n",
    "# scaler = MinMaxScaler(inputCol=\"features\",outputCol=\"scaledFeatures\")\n",
    "# scalerModel =  scaler.fit(transformed.select(\"features\"))\n",
    "# scaledData = scalerModel.transform(transformed)\n",
    "# def extract(row):\n",
    "#     return (row.pmid, )+tuple(row.scaledFeatures.toArray().tolist())\n",
    "# final_data = scaledData.select(\"pmid\",\"scaledFeatures\").rdd.map(extract).toDF(df.columns)\n",
    "# final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
