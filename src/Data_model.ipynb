{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql import *\n",
    "spark = SparkSession.builder.appName('Data_model').getOrCreate()\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------------+-----------------+-----------------+------------------+------------------+-----------------+-----------------+------------------+-----------------+-----------------+\n",
      "|_c0|Private|             Apps|           Accept|           Enroll|       F_Undergrad|       P_Undergrad|         Outstate|       Room_Board|         S_F_Ratio|           Expend|        Grad_Rate|\n",
      "+---+-------+-----------------+-----------------+-----------------+------------------+------------------+-----------------+-----------------+------------------+-----------------+-----------------+\n",
      "|  0|    Yes|7.415175109613295|7.117205503164344|6.582025138892826|7.9676267393338165|6.2878585601617845|8.914760527397261|8.101980731853192|2.9496883350525844|8.859647499714997|4.110873864173311|\n",
      "|  1|    Yes|7.690286020676768|7.562681246721884|6.240275845170769| 7.895063498091573| 7.113142108707088|9.415808631610384|8.771990436532242| 2.580216829592325|9.261793653565098| 4.04305126783455|\n",
      "|  2|    Yes|7.264730177929867|7.001245622069476|5.820082930352362|6.9440872082295275| 4.605170185988092|9.328212292571072|8.229777750081887| 2.631888840136646|9.075207697984686|4.007333185232471|\n",
      "|  3|    Yes|6.035481432524756|5.857933154483459|4.927253685157205| 6.236369590203704|4.1588830833596715|9.469700127423375|8.603554357064281| 2.163323025660538|9.853088594952276|  4.0943445622221|\n",
      "|  4|    Yes|5.267858159063328|4.990432586778736| 4.02535169073515| 5.521460917862246|  6.76849321164863| 8.93075873555827|8.323851131338817|2.5572273113676265|9.298625936843543|2.772588722239781|\n",
      "+---+-------+-----------------+-----------------+-----------------+------------------+------------------+-----------------+-----------------+------------------+-----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- Private: string (nullable = true)\n",
      " |-- Apps: double (nullable = true)\n",
      " |-- Accept: double (nullable = true)\n",
      " |-- Enroll: double (nullable = true)\n",
      " |-- F_Undergrad: double (nullable = true)\n",
      " |-- P_Undergrad: double (nullable = true)\n",
      " |-- Outstate: double (nullable = true)\n",
      " |-- Room_Board: double (nullable = true)\n",
      " |-- S_F_Ratio: double (nullable = true)\n",
      " |-- Expend: double (nullable = true)\n",
      " |-- Grad_Rate: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "normalizer_df = spark.read.csv('/home/ubuntu/BDAS_yang124/Datasets/normalizer_df_pandas.csv',inferSchema=True,header=True)\n",
    "normalizer_df.show(5)\n",
    "normalizer_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------------+-----------------+-----------------+------------------+------------------+-----------------+-----------------+------------------+-----------------+-----------------+--------------------+------------+\n",
      "|_c0|Private|             Apps|           Accept|           Enroll|       F_Undergrad|       P_Undergrad|         Outstate|       Room_Board|         S_F_Ratio|           Expend|        Grad_Rate|            features|PrivateIndex|\n",
      "+---+-------+-----------------+-----------------+-----------------+------------------+------------------+-----------------+-----------------+------------------+-----------------+-----------------+--------------------+------------+\n",
      "|  0|    Yes|7.415175109613295|7.117205503164344|6.582025138892826|7.9676267393338165|6.2878585601617845|8.914760527397261|8.101980731853192|2.9496883350525844|8.859647499714997|4.110873864173311|[7.41517510961329...|         0.0|\n",
      "|  1|    Yes|7.690286020676768|7.562681246721884|6.240275845170769| 7.895063498091573| 7.113142108707088|9.415808631610384|8.771990436532242| 2.580216829592325|9.261793653565098| 4.04305126783455|[7.69028602067676...|         0.0|\n",
      "|  2|    Yes|7.264730177929867|7.001245622069476|5.820082930352362|6.9440872082295275| 4.605170185988092|9.328212292571072|8.229777750081887| 2.631888840136646|9.075207697984686|4.007333185232471|[7.26473017792986...|         0.0|\n",
      "|  3|    Yes|6.035481432524756|5.857933154483459|4.927253685157205| 6.236369590203704|4.1588830833596715|9.469700127423375|8.603554357064281| 2.163323025660538|9.853088594952276|  4.0943445622221|[6.03548143252475...|         0.0|\n",
      "|  4|    Yes|5.267858159063328|4.990432586778736| 4.02535169073515| 5.521460917862246|  6.76849321164863| 8.93075873555827|8.323851131338817|2.5572273113676265|9.298625936843543|2.772588722239781|[5.26785815906332...|         0.0|\n",
      "+---+-------+-----------------+-----------------+-----------------+------------------+------------------+-----------------+-----------------+------------------+-----------------+-----------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[7.41517510961329...|  0.0|\n",
      "|[7.69028602067676...|  0.0|\n",
      "|[7.26473017792986...|  0.0|\n",
      "|[6.03548143252475...|  0.0|\n",
      "|[5.26785815906332...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "inputCols=['Apps',\n",
    " 'Accept',\n",
    " 'Enroll',\n",
    " 'F_Undergrad',\n",
    " 'P_Undergrad',\n",
    " 'Outstate',\n",
    " 'Room_Board',\n",
    " 'S_F_Ratio',\n",
    " 'Expend',\n",
    " 'Grad_Rate']\n",
    "assembler = VectorAssembler(\n",
    "  inputCols=inputCols,\n",
    "outputCol=\"features\")\n",
    "output = assembler.transform(normalizer_df)\n",
    "indexer = StringIndexer(inputCol=\"Private\", outputCol=\"PrivateIndex\")\n",
    "final_data_all = indexer.fit(output).transform(output)\n",
    "final_data_all.show(5)\n",
    "\n",
    "\n",
    "final_data = final_data_all.selectExpr(\"features\",\"`PrivateIndex` as label\")\n",
    "\n",
    "final_data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier,RandomForestClassifier,LogisticRegression,NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分割数据用于选择最佳模型，获取最佳参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要使用测模型构建，并且不适用参数并获取其评价分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTC\n",
      "0.8992599084623625\n",
      "RFC\n",
      "0.9784789171292236\n",
      "LR\n",
      "0.9738046547862493\n",
      "NB\n",
      "0.13253481351640864\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(maxIter=10)\n",
    "rfc = RandomForestClassifier(numTrees=10)\n",
    "dtc = DecisionTreeClassifier()\n",
    "nb = NaiveBayes()\n",
    "lr_original_model=lr.fit(train_data)\n",
    "rfc_original_model=rfc.fit(train_data)\n",
    "dtc_original_model=dtc.fit(train_data)\n",
    "nb_original_model=nb.fit(train_data)\n",
    "lr_original_predictions=lr_original_model.transform(test_data)\n",
    "rfc_original_predictions=rfc_original_model.transform(test_data)\n",
    "dtc_original_predictions=dtc_original_model.transform(test_data)\n",
    "nb_original_predictions=nb_original_model.transform(test_data)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "my_binary_eval = BinaryClassificationEvaluator()\n",
    "\n",
    "print(\"DTC\")\n",
    "print(my_binary_eval.evaluate(dtc_original_predictions))\n",
    "print(\"RFC\")\n",
    "print(my_binary_eval.evaluate(rfc_original_predictions))\n",
    "print(\"LR\")\n",
    "print(my_binary_eval.evaluate(lr_original_predictions))\n",
    "print(\"NB\")\n",
    "print(my_binary_eval.evaluate(nb_original_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "A single decision tree has an accuracy of: 91.15%\n",
      "----------------------------------------\n",
      "A random forest ensemble has an accuracy of: 94.25%\n",
      "----------------------------------------\n",
      "An ensemble using LR has an accuracy of: 94.69%\n",
      "----------------------------------------\n",
      "An ensemble using NB has an accuracy of: 76.11%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "acc_evaluator = MulticlassClassificationEvaluator( predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "dtc_acc = acc_evaluator.evaluate(dtc_original_predictions)\n",
    "rfc_acc = acc_evaluator.evaluate(rfc_original_predictions)\n",
    "lr_acc = acc_evaluator.evaluate(lr_original_predictions)\n",
    "nb_acc = acc_evaluator.evaluate(nb_original_predictions)\n",
    "\n",
    "print('-'*40)\n",
    "print('A single decision tree has an accuracy of: {0:2.2f}%'.format(dtc_acc*100))\n",
    "print('-'*40)\n",
    "print('A random forest ensemble has an accuracy of: {0:2.2f}%'.format(rfc_acc*100))\n",
    "print('-'*40)\n",
    "print('An ensemble using LR has an accuracy of: {0:2.2f}%'.format(lr_acc*100))\n",
    "print('-'*40)\n",
    "print('An ensemble using NB has an accuracy of: {0:2.2f}%'.format(nb_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数选择，模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({Param(parent='LogisticRegression_48f2a6224606139a780e', name='regParam', doc='regularization parameter (>= 0).'): 0.0001}, 0.9594424043096944)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "pipeline = Pipeline(stages=[lr])\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.regParam,[0.0001, 0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 1]).build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=10)  \n",
    "\n",
    "lr_best_model = crossval.fit(train_data)\n",
    "params = lr_best_model.getEstimatorParamMaps()\n",
    "avgMetrics = lr_best_model.avgMetrics\n",
    "all_params = list(zip(params, avgMetrics))\n",
    "best_param = sorted(all_params, key=lambda x: x[1], reverse=True)[0]\n",
    "\n",
    "print(best_param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({Param(parent='DecisionTreeClassifier_4f60a93d4b2a50f1a1e8', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5, Param(parent='DecisionTreeClassifier_4f60a93d4b2a50f1a1e8', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0}, 0.9053637460376946)\n"
     ]
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier()\n",
    "pipeline = Pipeline(stages=[dtc])\n",
    "paramGrid = ParamGridBuilder().addGrid(dtc.maxDepth,[5, 10]).addGrid(dtc.minInfoGain,[0,0.1,0.5,0.8, 1]).build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=10)  \n",
    "\n",
    "dtc_best_Model = crossval.fit(train_data)\n",
    "\n",
    "\n",
    "params = dtc_best_Model.getEstimatorParamMaps()\n",
    "avgMetrics = dtc_best_Model.avgMetrics\n",
    "all_params = list(zip(params, avgMetrics))\n",
    "best_param = sorted(all_params, key=lambda x: x[1], reverse=True)[0]\n",
    "\n",
    "print(best_param)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({Param(parent='DecisionTreeClassifier_4f60a93d4b2a50f1a1e8', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5, Param(parent='DecisionTreeClassifier_4f60a93d4b2a50f1a1e8', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0}, 0.9590244172833468)\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(numTrees=10)\n",
    "pipeline = Pipeline(stages=[rfc])\n",
    "paramGrid = ParamGridBuilder().addGrid(dtc.maxDepth,[5, 10]).addGrid(dtc.minInfoGain,[0,0.1,0.5,0.8, 1]).build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=10)  \n",
    "\n",
    "rfc_best_Model = crossval.fit(train_data)\n",
    "\n",
    "\n",
    "params = rfc_best_Model.getEstimatorParamMaps()\n",
    "avgMetrics = rfc_best_Model.avgMetrics\n",
    "all_params = list(zip(params, avgMetrics))\n",
    "best_param = sorted(all_params, key=lambda x: x[1], reverse=True)[0]\n",
    "\n",
    "print(best_param)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({Param(parent='NaiveBayes_4183ade2e25b44e97f3a', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 5}, 0.17847727785936476)\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes()\n",
    "pipeline = Pipeline(stages=[nb])\n",
    "paramGrid = ParamGridBuilder().addGrid(nb.smoothing,[1,3,5,8, 10,15]).build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=10)  \n",
    "\n",
    "nb_best_Model = crossval.fit(train_data)\n",
    "\n",
    "\n",
    "params = nb_best_Model.getEstimatorParamMaps()\n",
    "avgMetrics = nb_best_Model.avgMetrics\n",
    "all_params = list(zip(params, avgMetrics))\n",
    "best_param = sorted(all_params, key=lambda x: x[1], reverse=True)[0]\n",
    "\n",
    "print(best_param)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数选择后的模型进行评分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTC_best\n",
      "0.8992599084623625\n",
      "RFC_best\n",
      "0.9784789171292236\n",
      "LR_best\n",
      "0.9738046547862493\n",
      "NB_best\n",
      "0.13263219398188725\n",
      "----------------------------------------\n",
      "A single decision tree has an accuracy of: 91.15%\n",
      "----------------------------------------\n",
      "A random forest ensemble has an accuracy of: 94.25%\n",
      "----------------------------------------\n",
      "An ensemble using LR has an accuracy of: 94.69%\n",
      "----------------------------------------\n",
      "An ensemble using NB has an accuracy of: 76.11%\n"
     ]
    }
   ],
   "source": [
    "lr_best_predictions=lr_best_model.transform(test_data)\n",
    "rfc_best_predictions=rfc_best_Model.transform(test_data)\n",
    "dtc_best_predictions=dtc_best_Model.transform(test_data)\n",
    "nb_best_predictions=nb_best_Model.transform(test_data)\n",
    "\n",
    "my_binary_eval = BinaryClassificationEvaluator()\n",
    "\n",
    "print(\"DTC_best\")\n",
    "print(my_binary_eval.evaluate(dtc_best_predictions))\n",
    "\n",
    "\n",
    "print(\"RFC_best\")\n",
    "print(my_binary_eval.evaluate(rfc_best_predictions))\n",
    "\n",
    "print(\"LR_best\")\n",
    "print(my_binary_eval.evaluate(lr_best_predictions))\n",
    "\n",
    "print(\"NB_best\")\n",
    "print(my_binary_eval.evaluate(nb_best_predictions))\n",
    "\n",
    "dtc_acc = acc_evaluator.evaluate(dtc_best_predictions)\n",
    "rfc_acc = acc_evaluator.evaluate(rfc_best_predictions)\n",
    "lr_acc = acc_evaluator.evaluate(lr_best_predictions)\n",
    "nb_acc = acc_evaluator.evaluate(nb_best_predictions)\n",
    "\n",
    "print('-'*40)\n",
    "print('A single decision tree has an accuracy of: {0:2.2f}%'.format(dtc_acc*100))\n",
    "print('-'*40)\n",
    "print('A random forest ensemble has an accuracy of: {0:2.2f}%'.format(rfc_acc*100))\n",
    "print('-'*40)\n",
    "print('An ensemble using LR has an accuracy of: {0:2.2f}%'.format(lr_acc*100))\n",
    "print('-'*40)\n",
    "print('An ensemble using NB has an accuracy of: {0:2.2f}%'.format(nb_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR 模型\n",
      "Area under ROC = 0.973805\n",
      "Area under PR = 0.901227\n",
      "[[158.   5.]\n",
      " [  7.  56.]]\n",
      "[Overall]\tprecision = 0.9469026548672567 | recall = 0.9469026548672567 | F1 Measure = 0.9469026548672567\n",
      "[Class 0.0]\tprecision = 0.9575757575757575 | recall = 0.9693251533742331 | F1 Measure = 0.9634146341463414\n",
      "[Class 1.0]\tprecision = 0.9180327868852459 | recall = 0.8888888888888888 | F1 Measure = 0.9032258064516128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/mllib/evaluation.py:237: UserWarning: Deprecated in 2.0.0. Use accuracy.\n",
      "  warnings.warn(\"Deprecated in 2.0.0. Use accuracy.\")\n",
      "/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/mllib/evaluation.py:249: UserWarning: Deprecated in 2.0.0. Use accuracy.\n",
      "  warnings.warn(\"Deprecated in 2.0.0. Use accuracy.\")\n",
      "/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/mllib/evaluation.py:262: UserWarning: Deprecated in 2.0.0. Use accuracy.\n",
      "  warnings.warn(\"Deprecated in 2.0.0. Use accuracy.\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'opt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f0acf75fb7f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mlr_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_best_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/test.json'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DTC 模型\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'opt' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "def evaluate(predictionAndLabels):\n",
    "    log = {}\n",
    "\n",
    "    # Show Validation Score (AUROC)\n",
    "    evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "    log['AUROC'] = \"%f\" % evaluator.evaluate(predictionAndLabels)    \n",
    "    print(\"Area under ROC = {}\".format(log['AUROC']))\n",
    "\n",
    "    # Show Validation Score (AUPR)\n",
    "    evaluator = BinaryClassificationEvaluator(metricName='areaUnderPR')\n",
    "    log['AUPR'] = \"%f\" % evaluator.evaluate(predictionAndLabels)\n",
    "    print(\"Area under PR = {}\".format(log['AUPR']))\n",
    "\n",
    "    # Metrics\n",
    "    predictionRDD = predictionAndLabels.select(['label', 'prediction']) \\\n",
    "                            .rdd.map(lambda line: (line[1], line[0]))\n",
    "    metrics = MulticlassMetrics(predictionRDD)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    print(metrics.confusionMatrix().toArray())\n",
    "\n",
    "    # Overall statistics\n",
    "    log['precision'] = \"%s\" % metrics.precision()\n",
    "    log['recall'] = \"%s\" % metrics.recall()\n",
    "    log['F1 Measure'] = \"%s\" % metrics.fMeasure()\n",
    "    print(\"[Overall]\\tprecision = %s | recall = %s | F1 Measure = %s\" % \\\n",
    "            (log['precision'], log['recall'], log['F1 Measure']))\n",
    "\n",
    "    # Statistics by class\n",
    "    labels = [0.0, 1.0]\n",
    "    for label in sorted(labels):\n",
    "        log[label] = {}\n",
    "        log[label]['precision'] = \"%s\" % metrics.precision(label)\n",
    "        log[label]['recall'] = \"%s\" % metrics.recall(label)\n",
    "        log[label]['F1 Measure'] = \"%s\" % metrics.fMeasure(label, \n",
    "                                                           beta=1.0)\n",
    "        print(\"[Class %s]\\tprecision = %s | recall = %s | F1 Measure = %s\" \\\n",
    "                  % (label, log[label]['precision'], \n",
    "                    log[label]['recall'], log[label]['F1 Measure']))\n",
    "\n",
    "    return log\n",
    "\n",
    "print(\"LR 模型\")\n",
    "lr_log = evaluate(lr_best_predictions)\n",
    "\n",
    "print(\"DTC 模型\")\n",
    "evaluate(dtc_best_predictions)\n",
    "\n",
    "print(\"RF 模型\")\n",
    "evaluate(rfc_best_predictions)\n",
    "\n",
    "print(\"NB 模型\")\n",
    "evaluate(nb_best_predictions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
